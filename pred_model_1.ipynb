{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringIndexer, VectorAssembler\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassification\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MulticlassClassificationEvaluator\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ---------------------------------\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 1. Initialize Spark Session\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ---------------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/prototype_1/.venv/lib/python3.12/site-packages/pyspark/ml/__init__.py:31\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     Estimator,\n\u001b[32m     24\u001b[39m     Model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     UnaryTransformer,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     classification,\n\u001b[32m     33\u001b[39m     clustering,\n\u001b[32m     34\u001b[39m     evaluation,\n\u001b[32m     35\u001b[39m     feature,\n\u001b[32m     36\u001b[39m     fpm,\n\u001b[32m     37\u001b[39m     image,\n\u001b[32m     38\u001b[39m     recommendation,\n\u001b[32m     39\u001b[39m     regression,\n\u001b[32m     40\u001b[39m     stat,\n\u001b[32m     41\u001b[39m     tuning,\n\u001b[32m     42\u001b[39m     util,\n\u001b[32m     43\u001b[39m     linalg,\n\u001b[32m     44\u001b[39m     param,\n\u001b[32m     45\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TorchDistributor\n\u001b[32m     48\u001b[39m __all__ = [\n\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTransformer\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     50\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUnaryTransformer\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTorchDistributor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/prototype_1/.venv/lib/python3.12/site-packages/pyspark/ml/image.py:31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, NoReturn, Optional, cast\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdistutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Row, StructType, _create_row, _parse_datatype_json_string\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# ---------------------------------\n",
    "# 1. Initialize Spark Session\n",
    "# ---------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DiseasePredictionModel\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ---------------------------------\n",
    "# 2. Load the Realistic Healthcare Dataset\n",
    "# ---------------------------------\n",
    "data = spark.read.csv(\"/home/narayanan/Downloads/realistic_indian_healthcare_dataset.csv\", header=True, inferSchema=True)\n",
    "data.cache()\n",
    "data.printSchema()\n",
    "# The dataset should have columns like:\n",
    "# patient_id, region, city, hospital, age, gender, admission_date, discharge_date,\n",
    "# length_of_stay, symptoms, diagnosis, medication, outcome, lab_test_glucose, systolic_bp,\n",
    "# diastolic_bp, heart_rate, bmi, cholesterol, oxygen_saturation, respiratory_rate, temperature,\n",
    "# smoking_status, alcohol_use, insurance\n",
    "\n",
    "# ---------------------------------\n",
    "# 3. Preprocessing: Categorical Encoding & Feature Assembly\n",
    "# ---------------------------------\n",
    "# Categorical features to index (used as predictors)\n",
    "categorical_cols = [\"gender\", \"smoking_status\", \"alcohol_use\", \"insurance\", \"region\", \"city\", \"hospital\"]\n",
    "\n",
    "# Create StringIndexers for each categorical predictor\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\") for col in categorical_cols]\n",
    "\n",
    "# The target variable is 'diagnosis'. We convert it to a label.\n",
    "diagnosis_indexer = StringIndexer(inputCol=\"diagnosis\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "\n",
    "# List of numeric predictor columns\n",
    "numeric_cols = [\"age\", \"length_of_stay\", \"lab_test_glucose\", \"systolic_bp\",\n",
    "                \"diastolic_bp\", \"heart_rate\", \"bmi\", \"cholesterol\",\n",
    "                \"oxygen_saturation\", \"respiratory_rate\", \"temperature\"]\n",
    "\n",
    "# Final feature columns: numeric columns + indexed categorical columns\n",
    "feature_cols = numeric_cols + [col + \"_index\" for col in categorical_cols]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# ---------------------------------\n",
    "# 4. Define the Random Forest Classifier\n",
    "# ---------------------------------\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42, numTrees=50, maxDepth=10)\n",
    "\n",
    "# ---------------------------------\n",
    "# 5. Build the Pipeline\n",
    "# ---------------------------------\n",
    "stages = indexers + [diagnosis_indexer, assembler, rf]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# ---------------------------------\n",
    "# 6. Split Data and Train Model\n",
    "# ---------------------------------\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "train.cache()\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# ---------------------------------\n",
    "# 7. Evaluate the Model\n",
    "# ---------------------------------\n",
    "predictions = model.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Optionally, show sample predictions\n",
    "predictions.select(\"patient_id\", \"diagnosis\", \"prediction\").show(5, truncate=False)\n",
    "\n",
    "# ---------------------------------\n",
    "# 8. Save the Trained Model\n",
    "# ---------------------------------\n",
    "model.save(\"disease_prediction_model\")\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1,000,000 records with 50 diagnoses\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_records = 1_000_000  # Number of records to generate\n",
    "base_year = 2020  # Base year for temporal data\n",
    "\n",
    "# Expanded Medical Knowledge Base (50+ Diagnoses)\n",
    "diagnoses = {\n",
    "    # Cardiovascular (8)\n",
    "    'Hypertension': {'category': 'cardiovascular', 'severity': 2},\n",
    "    'Coronary Artery Disease': {'category': 'cardiovascular', 'severity': 3},\n",
    "    'Heart Failure': {'category': 'cardiovascular', 'severity': 4},\n",
    "    'Arrhythmia': {'category': 'cardiovascular', 'severity': 3},\n",
    "    'Peripheral Artery Disease': {'category': 'cardiovascular', 'severity': 3},\n",
    "    'Myocardial Infarction': {'category': 'cardiovascular', 'severity': 4},\n",
    "    'Cardiomyopathy': {'category': 'cardiovascular', 'severity': 4},\n",
    "    'Atherosclerosis': {'category': 'cardiovascular', 'severity': 3},\n",
    "    \n",
    "    # Respiratory (10)\n",
    "    'Asthma': {'category': 'respiratory', 'severity': 2},\n",
    "    'COPD': {'category': 'respiratory', 'severity': 3},\n",
    "    'Pneumonia': {'category': 'respiratory', 'severity': 3},\n",
    "    'COVID-19': {'category': 'respiratory', 'severity': 3},\n",
    "    'Tuberculosis': {'category': 'respiratory', 'severity': 4},\n",
    "    'Lung Cancer': {'category': 'respiratory', 'severity': 4},\n",
    "    'Pulmonary Embolism': {'category': 'respiratory', 'severity': 4},\n",
    "    'Bronchitis': {'category': 'respiratory', 'severity': 2},\n",
    "    'Pulmonary Fibrosis': {'category': 'respiratory', 'severity': 3},\n",
    "    'Sleep Apnea': {'category': 'respiratory', 'severity': 2},\n",
    "    \n",
    "    # Endocrine (8)\n",
    "    'Diabetes Type 1': {'category': 'endocrine', 'severity': 3},\n",
    "    'Diabetes Type 2': {'category': 'endocrine', 'severity': 3},\n",
    "    'Hypothyroidism': {'category': 'endocrine', 'severity': 2},\n",
    "    'Hyperthyroidism': {'category': 'endocrine', 'severity': 3},\n",
    "    'Cushing Syndrome': {'category': 'endocrine', 'severity': 3},\n",
    "    'Addison Disease': {'category': 'endocrine', 'severity': 3},\n",
    "    'Osteoporosis': {'category': 'endocrine', 'severity': 2},\n",
    "    'Metabolic Syndrome': {'category': 'endocrine', 'severity': 2},\n",
    "    \n",
    "    # Gastrointestinal (8)\n",
    "    'Gastritis': {'category': 'gastrointestinal', 'severity': 2},\n",
    "    'GERD': {'category': 'gastrointestinal', 'severity': 2},\n",
    "    'IBD': {'category': 'gastrointestinal', 'severity': 3},\n",
    "    'Cirrhosis': {'category': 'gastrointestinal', 'severity': 4},\n",
    "    'Pancreatitis': {'category': 'gastrointestinal', 'severity': 3},\n",
    "    'Hepatitis': {'category': 'gastrointestinal', 'severity': 3},\n",
    "    'Colorectal Cancer': {'category': 'gastrointestinal', 'severity': 4},\n",
    "    'Gallstones': {'category': 'gastrointestinal', 'severity': 2},\n",
    "    \n",
    "    # Neurological (8)\n",
    "    'Migraine': {'category': 'neurological', 'severity': 2},\n",
    "    'Epilepsy': {'category': 'neurological', 'severity': 3},\n",
    "    'Stroke': {'category': 'neurological', 'severity': 4},\n",
    "    'Alzheimer': {'category': 'neurological', 'severity': 4},\n",
    "    'Parkinson': {'category': 'neurological', 'severity': 4},\n",
    "    'Multiple Sclerosis': {'category': 'neurological', 'severity': 3},\n",
    "    'Neuropathy': {'category': 'neurological', 'severity': 3},\n",
    "    'Brain Tumor': {'category': 'neurological', 'severity': 4},\n",
    "    \n",
    "    # Additional Categories (8+)\n",
    "    'Chronic Kidney Disease': {'category': 'renal', 'severity': 3},\n",
    "    'Rheumatoid Arthritis': {'category': 'musculoskeletal', 'severity': 3},\n",
    "    'Osteoarthritis': {'category': 'musculoskeletal', 'severity': 2},\n",
    "    'Sepsis': {'category': 'infectious', 'severity': 4},\n",
    "    'HIV/AIDS': {'category': 'infectious', 'severity': 4},\n",
    "    'Malaria': {'category': 'infectious', 'severity': 3},\n",
    "    'Anemia': {'category': 'hematological', 'severity': 2},\n",
    "    'Leukemia': {'category': 'hematological', 'severity': 4}\n",
    "}\n",
    "\n",
    "# Symptom Map\n",
    "symptom_map = {\n",
    "    'cardiovascular': [\n",
    "        'chest pain', 'palpitations', 'shortness of breath',\n",
    "        'fatigue', 'dizziness', 'leg swelling', 'syncope'\n",
    "    ],\n",
    "    'respiratory': [\n",
    "        'cough', 'dyspnea', 'chest tightness', 'wheezing',\n",
    "        'sputum production', 'hemoptysis', 'nasal congestion'\n",
    "    ],\n",
    "    'endocrine': [\n",
    "        'fatigue', 'polyuria', 'polydipsia', 'weight changes',\n",
    "        'heat intolerance', 'cold intolerance', 'skin changes'\n",
    "    ],\n",
    "    'gastrointestinal': [\n",
    "        'abdominal pain', 'nausea', 'vomiting', 'diarrhea',\n",
    "        'constipation', 'bloating', 'rectal bleeding'\n",
    "    ],\n",
    "    'neurological': [\n",
    "        'headache', 'dizziness', 'numbness', 'tingling',\n",
    "        'muscle weakness', 'seizures', 'vision changes'\n",
    "    ],\n",
    "    'renal': [\n",
    "        'edema', 'urinary changes', 'foamy urine',\n",
    "        'flank pain', 'frequency', 'nocturia'\n",
    "    ],\n",
    "    'musculoskeletal': [\n",
    "        'joint pain', 'muscle pain', 'stiffness',\n",
    "        'reduced mobility', 'swelling', 'redness'\n",
    "    ],\n",
    "    'infectious': [\n",
    "        'fever', 'chills', 'sweating', 'malaise',\n",
    "        'lymphadenopathy', 'rash', 'weight loss'\n",
    "    ],\n",
    "    'hematological': [\n",
    "        'pallor', 'easy bruising', 'bleeding',\n",
    "        'petechiae', 'bone pain', 'fatigue'\n",
    "    ],\n",
    "    'common': [  # Symptoms that can appear across categories\n",
    "        'fever', 'fatigue', 'weight loss', 'loss of appetite',\n",
    "        'sweating', 'malaise', 'weakness'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Helper Function: Generate Symptoms\n",
    "def generate_symptoms(category, severity):\n",
    "    \"\"\"Generate realistic symptoms based on category and severity\"\"\"\n",
    "    # Base symptoms from category\n",
    "    base_symptoms = np.random.choice(\n",
    "        symptom_map[category],\n",
    "        size=np.random.randint(1, 4),  # 1-3 base symptoms\n",
    "        replace=False\n",
    "    ).tolist()\n",
    "    \n",
    "    # Add common symptoms with 30% probability\n",
    "    if np.random.rand() < 0.3:\n",
    "        common = np.random.choice(symptom_map['common'], 1)\n",
    "        base_symptoms.extend(common)\n",
    "    \n",
    "    # Add severity-specific symptoms\n",
    "    if severity >= 3:\n",
    "        base_symptoms.append('severe ' + np.random.choice(base_symptoms))\n",
    "    \n",
    "    # Add 5% chance of unrelated symptom\n",
    "    if np.random.rand() < 0.05:\n",
    "        other_cat = np.random.choice(list(symptom_map.keys()))\n",
    "        base_symptoms.append(np.random.choice(symptom_map[other_cat]))\n",
    "    \n",
    "    return ', '.join(list(set(base_symptoms)))  # Ensure uniqueness\n",
    "\n",
    "# Helper Function: Generate Lab Values\n",
    "def generate_lab_values(diagnosis, category, severity):\n",
    "    \"\"\"Generate lab values with disease-specific adjustments\"\"\"\n",
    "    # Base values\n",
    "    labs = pd.DataFrame({\n",
    "        'glucose': np.clip(np.random.normal(100, 15, len(diagnosis)), 70, 300),\n",
    "        'systolic_bp': np.clip(np.random.normal(120, 15, len(diagnosis)), 90, 200),\n",
    "        'diastolic_bp': np.clip(np.random.normal(80, 10, len(diagnosis)), 60, 120),\n",
    "        'wbc': np.clip(np.random.lognormal(2, 0.3, len(diagnosis)), 3, 20)\n",
    "    })\n",
    "    \n",
    "    # Disease-specific adjustments\n",
    "    conditions = {\n",
    "        'diabetes': (diagnosis.str.contains('Diabetes')),\n",
    "        'hypertension': (diagnosis == 'Hypertension'),\n",
    "        'infection': (diagnosis.isin(['Pneumonia', 'COVID-19', 'Sepsis']))\n",
    "    }\n",
    "    \n",
    "    # Apply adjustments using numpy.where\n",
    "    labs['glucose'] = np.where(conditions['diabetes'],\n",
    "                              labs['glucose'] * 1.5,\n",
    "                              labs['glucose'])\n",
    "    \n",
    "    labs['systolic_bp'] = np.where(conditions['hypertension'],\n",
    "                                 labs['systolic_bp'] + 20,\n",
    "                                 labs['systolic_bp'])\n",
    "    \n",
    "    labs['wbc'] = np.where(conditions['infection'],\n",
    "                          labs['wbc'] * 1.8,\n",
    "                          labs['wbc'])\n",
    "    \n",
    "    return labs.round(1)\n",
    "\n",
    "# Generate Base Data\n",
    "df = pd.DataFrame({\n",
    "    'patient_id': [f'P{str(x).zfill(8)}' for x in range(n_records)],\n",
    "    'age': np.clip(np.random.normal(50, 15, n_records), 18, 100).astype(int),\n",
    "    'gender': np.random.choice(['M', 'F'], n_records, p=[0.51, 0.49]),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_records),\n",
    "    'bmi': np.clip(np.random.normal(25, 5, n_records), 15, 45)\n",
    "})\n",
    "\n",
    "# Assign Diagnoses\n",
    "diag_list = list(diagnoses.keys())\n",
    "weights = [d['severity']**2 for d in diagnoses.values()]  # Weight by severity^2\n",
    "df['diagnosis'] = np.random.choice(diag_list, n_records, p=np.array(weights)/sum(weights))\n",
    "\n",
    "# Add Disease Metadata\n",
    "meta = pd.DataFrame.from_dict(diagnoses, orient='index').reset_index()\n",
    "df = df.merge(meta, left_on='diagnosis', right_on='index').drop(columns='index')\n",
    "\n",
    "# Generate Symptoms\n",
    "df['symptoms'] = df.apply(\n",
    "    lambda row: generate_symptoms(row['category'], row['severity']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Generate Lab Values\n",
    "labs = generate_lab_values(df['diagnosis'], df['category'], df['severity'])\n",
    "df = pd.concat([df, labs], axis=1)\n",
    "\n",
    "# Temporal Data\n",
    "admit_dates = pd.to_datetime(np.random.choice(\n",
    "    pd.date_range(f'{base_year}-01-01', f'{base_year+3}-12-31'), n_records\n",
    "))\n",
    "df['admit_date'] = admit_dates\n",
    "df['los'] = np.clip(np.random.poisson(df['severity'] * 2 + np.random.normal(3, 1, n_records)), 1, 30)\n",
    "df['discharge_date'] = df['admit_date'] + pd.to_timedelta(df['los'], unit='D')\n",
    "\n",
    "# Medications (Example for 3 categories)\n",
    "df['medication'] = np.select(\n",
    "    [\n",
    "        df['category'] == 'cardiovascular',\n",
    "        df['category'] == 'respiratory',\n",
    "        df['category'] == 'endocrine'\n",
    "    ],\n",
    "    [\n",
    "        np.random.choice(['Lisinopril', 'Amlodipine', 'Metoprolol'], n_records),\n",
    "        np.random.choice(['Albuterol', 'Prednisone', 'Montelukast'], n_records),\n",
    "        np.random.choice(['Metformin', 'Insulin', 'Levothyroxine'], n_records)\n",
    "    ],\n",
    "    default='Other'\n",
    ")\n",
    "\n",
    "# Define a function to calculate outcome probabilities based on severity\n",
    "def calculate_outcome_probs(severity):\n",
    "    base_probs = np.array([0.5, 0.3, 0.15, 0.04, 0.01])  # Base probabilities\n",
    "    severity_factor = severity / 4  # Adjust based on severity\n",
    "    adjusted_probs = base_probs * (1 + severity_factor)  # Scale probabilities\n",
    "    adjusted_probs /= adjusted_probs.sum()  # Normalize to sum to 1\n",
    "    return adjusted_probs\n",
    "\n",
    "# Apply the function to generate outcomes\n",
    "df['outcome'] = df['severity'].apply(\n",
    "    lambda s: np.random.choice(\n",
    "        ['Recovered', 'Stable', 'Deteriorated', 'Critical', 'Deceased'],\n",
    "        p=calculate_outcome_probs(s)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save in Parquet format for efficiency\n",
    "df.to_parquet('million_patients.parquet', engine='pyarrow')\n",
    "print(f\"Generated {len(df):,} records with {len(diagnoses)} diagnoses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringIndexer, VectorAssembler, OneHotEncoder, Tokenizer, CountVectorizer\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassification\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Initialize Spark\u001b[39;00m\n\u001b[32m      7\u001b[39m spark = SparkSession.builder.appName(\u001b[33m\"\u001b[39m\u001b[33mDiseasePrediction\u001b[39m\u001b[33m\"\u001b[39m).getOrCreate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/prototype_1/.venv/lib/python3.12/site-packages/pyspark/ml/__init__.py:31\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     Estimator,\n\u001b[32m     24\u001b[39m     Model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     UnaryTransformer,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     classification,\n\u001b[32m     33\u001b[39m     clustering,\n\u001b[32m     34\u001b[39m     evaluation,\n\u001b[32m     35\u001b[39m     feature,\n\u001b[32m     36\u001b[39m     fpm,\n\u001b[32m     37\u001b[39m     image,\n\u001b[32m     38\u001b[39m     recommendation,\n\u001b[32m     39\u001b[39m     regression,\n\u001b[32m     40\u001b[39m     stat,\n\u001b[32m     41\u001b[39m     tuning,\n\u001b[32m     42\u001b[39m     util,\n\u001b[32m     43\u001b[39m     linalg,\n\u001b[32m     44\u001b[39m     param,\n\u001b[32m     45\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TorchDistributor\n\u001b[32m     48\u001b[39m __all__ = [\n\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTransformer\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     50\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUnaryTransformer\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTorchDistributor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/prototype_1/.venv/lib/python3.12/site-packages/pyspark/ml/image.py:31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, NoReturn, Optional, cast\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdistutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Row, StructType, _create_row, _parse_datatype_json_string\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"DiseasePrediction\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df = spark.read.parquet(\"million_patients.parquet\")\n",
    "\n",
    "# Preprocess symptoms (text to features)\n",
    "tokenizer = Tokenizer(inputCol=\"symptoms\", outputCol=\"symptoms_tokens\")\n",
    "cv = CountVectorizer(inputCol=\"symptoms_tokens\", outputCol=\"symptoms_vec\")\n",
    "\n",
    "# Encode categorical features\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_index\") \n",
    "    for c in [\"gender\", \"region\"]\n",
    "]\n",
    "encoder = OneHotEncoder(inputCols=[f\"{c}_index\" for c in [\"gender\", \"region\"]], \n",
    "                       outputCols=[f\"{c}_encoded\" for c in [\"gender\", \"region\"]])\n",
    "\n",
    "# Assemble features\n",
    "feature_cols = [\"symptoms_vec\", \"age\", \"bmi\", \"glucose\", \"systolic_bp\"] + \\\n",
    "               [f\"{c}_encoded\" for c in [\"gender\", \"region\"]]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Define model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"diagnosis_index\", \n",
    "                           numTrees=100, maxDepth=10)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer, \n",
    "    cv, \n",
    "    *indexers, \n",
    "    encoder, \n",
    "    assembler, \n",
    "    StringIndexer(inputCol=\"diagnosis\", outputCol=\"diagnosis_index\"), \n",
    "    rf\n",
    "])\n",
    "\n",
    "# Train/test split\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.transform(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
